import random

class QLearningAgent:
    def __init__(self, epsilon=0.1, difficulty='normal'):
        self.q_table = {}
        self.epsilon = epsilon
        self.alpha = 0.1
        self.gamma = 0.9
        self.difficulty = difficulty  # easy, normal, hard, master


    def get_q(self, state, action):
        state = str(state)  # âœ… í•­ìƒ ë¬¸ìì—´ë¡œ ë³€í™˜
        return self.q_table.get((state, action), 0.0)

    def choose_action(self, state, actions):
        import random

        # ì‰¬ìš´ ë‚œì´ë„ëŠ” ì¼ì • í™•ë¥ ë¡œ ì™„ì „ ëœë¤
        if self.difficulty == 'easy' and random.random() < 0.3:
            return random.choice(actions)

        if self.difficulty == 'normal' and random.random() < 0.1:
            return random.choice(actions)

        # ì›ë˜ QëŸ¬ë‹ ë°©ì‹
        if random.random() < self.epsilon:
            return random.choice(actions)
        
        q_values = [self.q_table.get((state, a), 0) for a in actions]
        max_q = max(q_values)
        max_actions = [a for a, q in zip(actions, q_values) if q == max_q]
        return random.choice(max_actions)


    def learn(self, state, action, reward, next_state, next_actions):
        state = str(state)
        next_state = str(next_state)

        old_q = self.get_q(state, action)
        future_q = max([self.get_q(next_state, a) for a in next_actions]) if next_actions else 0
        new_q = old_q + self.alpha * (reward + self.gamma * future_q - old_q)
        self.q_table[(state, action)] = new_q

    # âœ… (ì„ íƒ) Qê°’ í™•ì¸ìš© í•¨ìˆ˜
    def print_sample_q(self, n=5):
        print("ğŸ” Q-table ì˜ˆì‹œ:")
        for i, ((state, action), q) in enumerate(self.q_table.items()):
            if i >= n:
                break
            print(f"State: {state}, Action: {action}, Q: {q:.2f}")
